import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os

BASE_URL = "https://www.10000recipe.com"

def get_recipe_links_from_page(base_url, page=1):
    """íŠ¹ì • í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ ë§í¬ ì¶”ì¶œ"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # í˜ì´ì§€ ë²ˆí˜¸ê°€ 1ì´ë©´ ê¸°ë³¸ URL, ì•„ë‹ˆë©´ page íŒŒë¼ë¯¸í„° ì¶”ê°€
    if page == 1:
        page_url = base_url
    else:
        page_url = f"{base_url}&page={page}"
    
    try:
        res = requests.get(page_url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        print(f"í˜ì´ì§€ {page} ë¡œë”© ì„±ê³µ: {page_url}")
        
        # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ë ˆì‹œí”¼ ë§í¬ ì°¾ê¸°
        links = []
        
        # ë°©ë²• 1: ë§Œê°œì˜ë ˆì‹œí”¼ íŠ¹í™” ì…€ë ‰í„°ë“¤
        selectors_to_try = [
            "a[href*='/recipe/']",
            ".common_sp_link",
            ".thumbnail a",
            ".recipe_link",
            ".list_recipe a"
        ]
        
        for selector in selectors_to_try:
            for a in soup.select(selector):
                href = a.get("href")
                if href and "/recipe/" in href:
                    if href.startswith("/"):
                        full_url = BASE_URL + href
                    else:
                        full_url = href
                    if full_url not in links:
                        links.append(full_url)
        
        # ë°©ë²• 2: ì¼ë°˜ì ì¸ ë ˆì‹œí”¼ ë§í¬ íŒ¨í„´ (ë°±ì—…)
        if not links:
            for a in soup.find_all("a", href=True):
                href = a.get("href")
                if href and "/recipe/" in href:
                    if href.startswith("/"):
                        full_url = BASE_URL + href
                    else:
                        full_url = href
                    if full_url not in links:
                        links.append(full_url)
        
        print(f"í˜ì´ì§€ {page}ì—ì„œ ë°œê²¬ëœ ë ˆì‹œí”¼ ë§í¬ ìˆ˜: {len(links)}")
        return links
        
    except requests.RequestException as e:
        print(f"í˜ì´ì§€ {page} ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f"í˜ì´ì§€ {page} íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []


def get_all_recipe_links(base_url, max_recipes=100):
    """ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ìµœëŒ€ max_recipesê°œì˜ ë ˆì‹œí”¼ ë§í¬ ìˆ˜ì§‘"""
    all_links = []
    page = 1
    
    while len(all_links) < max_recipes:
        print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
        page_links = get_recipe_links_from_page(base_url, page)
        
        if not page_links:
            print(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break
        
        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
        new_links = [link for link in page_links if link not in all_links]
        all_links.extend(new_links)
        
        print(f"í˜ì´ì§€ {page}ì—ì„œ {len(new_links)}ê°œ ìƒˆë¡œìš´ ë§í¬ ì¶”ê°€ (ì´ {len(all_links)}ê°œ)")
        
        # ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
        if len(all_links) >= max_recipes:
            all_links = all_links[:max_recipes]
            break
            
        page += 1
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€ (ìµœëŒ€ 50í˜ì´ì§€ê¹Œì§€ë§Œ)
        if page > 50:
            print("ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break
            
        # í˜ì´ì§€ ê°„ ëŒ€ê¸°ì‹œê°„
        time.sleep(0.5)
    
    print(f"\nì´ {len(all_links)}ê°œì˜ ë ˆì‹œí”¼ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_links


def get_recipe_detail(recipe_url):
    """ë ˆì‹œí”¼ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ì¬ë£Œ, ì¡°ë¦¬ìˆœì„œ í¬ë¡¤ë§"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        res = requests.get(recipe_url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
        title = None
        title_selectors = [
            ".view2_summary h3",
            "h1.recipe-title",
            ".recipe_title",
            "h1",
            ".title"
        ]
        
        for selector in title_selectors:
            title_tag = soup.select_one(selector)
            if title_tag:
                title = title_tag.get_text(strip=True)
                break
        
        if not title:
            # í˜ì´ì§€ íƒ€ì´í‹€ì—ì„œ ì¶”ì¶œ ì‹œë„
            page_title = soup.find("title")
            if page_title:
                title = page_title.get_text(strip=True).replace("ë§Œê°œì˜ë ˆì‹œí”¼", "").strip()
            else:
                raise ValueError("ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # ì¬ë£Œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
        ingredients = []
        ingredient_selectors = [
            ".ready_ingre3 li",
            ".recipe_ingredient li", 
            ".ingredient-list li",
            ".ingre_list li",
            ".ready_ingre li",
            ".view2_ingre li",
            ".ingre_box li"
        ]
        
        for selector in ingredient_selectors:
            ingredient_elements = soup.select(selector)
            if ingredient_elements:
                print(f"ğŸ” ì¬ë£Œ ì…€ë ‰í„° '{selector}' ì‚¬ìš© - {len(ingredient_elements)}ê°œ ë°œê²¬")
                for ing in ingredient_elements:
                    txt = ing.get_text(" ", strip=True)
                    if txt and len(txt) > 1 and txt not in ingredients:  # ìµœì†Œ 2ê¸€ì ì´ìƒ
                        ingredients.append(txt)
                break
        
        # ì¬ë£Œê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if not ingredients:
            print("ğŸ” ëŒ€ì²´ ì¬ë£Œ ì¶”ì¶œ ë°©ë²• ì‹œë„...")
            for ing in soup.select("*"):
                if ing.name and "ingre" in ing.get("class", []):
                    txt = ing.get_text(" ", strip=True)
                    if txt and len(txt) > 1:
                        ingredients.append(txt)

        # ì¡°ë¦¬ìˆœì„œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
        steps = []
        step_selectors = [
            ".view_step_cont",
            ".recipe_step",
            ".cooking-step", 
            ".step_cont",
            ".view_step .step_txt",
            ".manual_txt",
            ".step_list li"
        ]
        
        for selector in step_selectors:
            step_elements = soup.select(selector)
            if step_elements:
                print(f"ğŸ” ì¡°ë¦¬ìˆœì„œ ì…€ë ‰í„° '{selector}' ì‚¬ìš© - {len(step_elements)}ê°œ ë°œê²¬")
                for step in step_elements:
                    txt = step.get_text(" ", strip=True)
                    if txt and len(txt) > 5 and txt not in steps:  # ìµœì†Œ 5ê¸€ì ì´ìƒ
                        steps.append(txt)
                break
        
        # ì¡°ë¦¬ìˆœì„œê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if not steps:
            print("ğŸ” ëŒ€ì²´ ì¡°ë¦¬ìˆœì„œ ì¶”ì¶œ ë°©ë²• ì‹œë„...")
            for step in soup.select("*"):
                if step.name and any(word in step.get("class", []) for word in ["step", "manual", "cook"]):
                    txt = step.get_text(" ", strip=True)
                    if txt and len(txt) > 5:
                        steps.append(txt)

        # ì¡°ë¦¬ìˆœì„œì— ë²ˆí˜¸ ì¶”ê°€
        numbered_steps = []
        if steps:
            for i, step in enumerate(steps, 1):
                numbered_steps.append(f"{i}. {step}")
        
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"ğŸ” ë””ë²„ê¹… - ì œëª©: {title[:50] if title else 'None'}...")
        print(f"ğŸ” ë””ë²„ê¹… - ì¬ë£Œ ìˆ˜: {len(ingredients)}")
        print(f"ğŸ” ë””ë²„ê¹… - ì¡°ë¦¬ìˆœì„œ ìˆ˜: {len(steps)}")
        
        # ëª¨ë“  í•„ìˆ˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not title or not ingredients or not steps:
            print(f"âš ï¸ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ - ì œëª©: {bool(title)}, ì¬ë£Œ: {bool(ingredients)}, ì¡°ë¦¬ìˆœì„œ: {bool(steps)}")
            return None
            
        return {
            "title": title,
            "url": recipe_url,
            "ingredients": "; ".join(ingredients),
            "steps": " | ".join(numbered_steps)
        }
        
    except requests.RequestException as e:
        print(f"í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜ ({recipe_url}): {e}")
        return None
    except Exception as e:
        print(f"íŒŒì‹± ì˜¤ë¥˜ ({recipe_url}): {e}")
        return None


if __name__ == "__main__":
    # íƒ€ê²Ÿ ì‚¬ì´íŠ¸ URL ìˆ˜ì •
    base_url = "https://www.10000recipe.com/issue/view.html?cid=9999scrap"
    target_count = 200
    
    print("ğŸš€ ë§Œê°œì˜ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œì‘...")
    print(f"íƒ€ê²Ÿ URL: {base_url}")
    print(f"ëª©í‘œ ë ˆì‹œí”¼ ìˆ˜: {target_count}ê°œ")
    
    # ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ ë§í¬ ìˆ˜ì§‘
    recipe_links = get_all_recipe_links(base_url, target_count)

    if not recipe_links:
        print("âŒ ë ˆì‹œí”¼ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì´ë‚˜ ì…€ë ‰í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit(1)

    # ë°ì´í„° ìˆ˜ì§‘
    data = []
    success_count = 0
    skip_count = 0  # ë°ì´í„° ëˆ„ë½ìœ¼ë¡œ ê±´ë„ˆë›´ ìˆ˜
    
    print(f"\nğŸ“Š {len(recipe_links)}ê°œ ë ˆì‹œí”¼ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    
    for i, link in enumerate(recipe_links):
        if success_count >= target_count:  # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¢…ë£Œ
            break
            
        print(f"[{i+1}/{len(recipe_links)}] ì²˜ë¦¬ ì¤‘: {link}")
        
        detail = get_recipe_detail(link)
        if detail:
            data.append(detail)
            success_count += 1
            print(f"âœ… {success_count}ê°œ ì™„ë£Œ: {detail['title']}")
        else:
            # detailì´ Noneì´ë©´ ë°ì´í„° ëˆ„ë½ ë˜ëŠ” ì˜¤ë¥˜
            skip_count += 1
            print(f"â­ï¸ ë°ì´í„° ëˆ„ë½ ë˜ëŠ” ì˜¤ë¥˜ë¡œ ê±´ë„ˆëœ€ ({skip_count}ê°œ)")
            
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ (1ì´ˆ ëŒ€ê¸°)
        time.sleep(1)

    print(f"\nğŸ“ˆ ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"â­ï¸ ê±´ë„ˆëœ€: {skip_count}ê°œ")

    if data:
        # ë°ì´í„° í´ë” ìƒì„±
        data_dir = "./crawling/data"
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
            
        # CSV ì €ì¥ (ì™„ì „í•œ ë°ì´í„°ë§Œ)
        df = pd.DataFrame(data, columns=["title", "url", "ingredients", "steps"])
        output_file = os.path.join(data_dir, "recipes.csv")
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"ğŸ“‚ {output_file} íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(data)}ê°œ ì™„ì „í•œ ë ˆì‹œí”¼ ë°ì´í„° ìˆ˜ì§‘ë¨")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        print("\nğŸ” ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ:")
        for i, recipe in enumerate(data[:3]):
            print(f"\n[{i+1}] {recipe['title']}")
            print(f"URL: {recipe['url']}")
            print(f"ì¬ë£Œ: {recipe['ingredients'][:100]}...")
            print(f"ì¡°ë¦¬ë²•: {recipe['steps'][:100]}...")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ì™„ì „í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")